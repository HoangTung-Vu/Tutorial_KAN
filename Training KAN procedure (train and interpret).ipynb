{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import torch\n",
    "from kan.utils import create_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE A kan KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kan = KAN([2,5,1], device=device, grid = 3, k = 3, seed = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE A DATASET FROM FUNCTION DEFINED AS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "#f(x,y) = e^(sin(pi*x)+y^2) + noise\n",
    "f = lambda x : torch.exp(torch.cos(torch.pi * x[:, [0]]) + x[:, [1]]**2) + torch.randn_like(x[:,[1]])*0.1\n",
    "dataset = create_dataset(f, n_var=2, device = device, train_num = 1000, test_num = 800)\n",
    "\n",
    "train = dataset['train_input']\n",
    "train_target = dataset['train_label']\n",
    "print(train.shape)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LET THE kan SEE THE DATA AND PLOT THE INITIAL kan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kan(dataset['train_input'])\n",
    "kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN WITH L1 REGULARIZATION FOR LATER SPARSIFICATION\n",
    "#### L1 norm of an activation function $\\phi$ over its $N_p$ input : $$|\\phi|_1 = \\frac{1}{N_p} \\times \\sum_{s=1}^{N_p}{\\phi (x_s)} $$\n",
    "#### For a KAN layer $ \\ Phi $ with $n_{in}$ inputs and $n_{out}$ outputs, L1 norm of the layer is the sum of L1 norms of all activation function in that layer : $$|\\Phi|_1 = \\sum_{i=1}^{n_{in}}{\\sum_{j=1}^{n_{j}}{|\\phi_{i,j}|_1}} $$\n",
    "#### Entropy of KAN Layer $\\Phi$ : $$ S(\\Phi) = \\sum_{i=1}^{n_{in}}{\\sum_{j=1}^{n_{j}}}{\\frac{|\\phi_{i,j}|_1}{|\\Phi|_1} \\times log(\\frac{|\\phi_{i,j}|_1}{|\\Phi|_1})} $$\n",
    "\n",
    "#### Total loss of KAN with L layers : $$l_{total} = l_{pred} + \\lambda \\times (\\lambda_1 \\sum_{l=0}^{L-1}{|\\Phi|_1} +\\lambda_2 \\sum_{l=0}^{L-1}{S(\\Phi_l)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = kan.fit(dataset, opt=\"LBFGS\", steps=40, lamb=0.01, lamb_l1=10., lamb_entropy=10.)\n",
    "plt.plot(res['train_loss'])\n",
    "plt.show()\n",
    "kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUNING (SPARSIFICATION)\n",
    "##### For a node of layer $l^{th}$ , $i^{th}$ neuron : \n",
    "##### $$ I_{l,i} = max_k(|\\phi_{l-1, i, k}|1)$$ \n",
    "##### $$ O_{l,i} = max_j(|\\phi_{l+1, i, j}|1)$$\n",
    "##### A node is considered \"important\" if both score are greater than a threshold $\\theta$ (set = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kan = kan.prune(node_th=0.01)\n",
    "kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTINUE TRAINING ON SPARSIFIED KAN (AND EXTEND GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kan.fit(dataset, opt= \"LBFGS\", steps = 10)\n",
    "kan = kan.refine(10)    \n",
    "res2 = kan.fit(dataset, opt=\"LBFGS\", steps=10)\n",
    "plt.plot(res2['train_loss'])\n",
    "plt.show()\n",
    "kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET ACTIVATION FUNCTIONS (SPLINES -> SYMBOLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"auto\" # \"manual\"\n",
    "from kan.utils import SYMBOLIC_LIB\n",
    "if mode == \"manual\":\n",
    "    # manual mode\n",
    "    kan.fix_symbolic(0,0,0,'sin')\n",
    "    kan.fix_symbolic(0,1,0,'x^2')\n",
    "    kan.fix_symbolic(1,0,0,'exp')\n",
    "elif mode == \"auto\":\n",
    "    # automatic mode\n",
    "    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']\n",
    "    kan.auto_symbolic(lib=lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTINUE TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res3 = kan.fit(dataset, opt=\"LBFGS\", steps=15)\n",
    "plt.plot(res3['train_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTAIN SYMBOLIC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan.utils import ex_round\n",
    "ex_round(kan.symbolic_formula()[0][0],4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
